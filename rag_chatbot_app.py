import streamlit as st
import tempfile
import os
import torch
import chromadb
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from langchain_chroma import Chroma
from langchain_huggingface.llms import HuggingFacePipeline
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from transformers import BitsAndBytesConfig
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
import time
from operator import itemgetter
from utils.logging_utils import logger

# Session state initialization
if 'rag_chain' not in st.session_state:
    st.session_state.rag_chain = None
if 'models_loaded' not in st.session_state:
    st.session_state.models_loaded = False
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = None
if 'llm' not in st.session_state:
    st.session_state.llm = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'pdf_processed' not in st.session_state:
    st.session_state.pdf_processed = False
if 'pdf_name' not in st.session_state:
    st.session_state.pdf_name = ""

# Functions
@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(model_name="bkai-foundation-models/vietnamese-bi-encoder")

def get_chroma_client(allow_reset=False):
    """Get a Chroma client for vector database operations."""
    # Use PersistentClient for persistent storage
    return chromadb.PersistentClient(settings=chromadb.Settings(allow_reset=allow_reset))

@st.cache_resource  
def load_llm():
    MODEL_NAME = "lmsys/vicuna-7b-v1.5"

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4"
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto"
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    model_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        pad_token_id=tokenizer.eos_token_id,
        device_map="auto"
    )
    
    return HuggingFacePipeline(pipeline=model_pipeline)


def build_prompt_fromhub_ragprompt():
    """Build a prompt for the RAG chain."""
    # Load the prompt from the hub: "rlm/rag-prompt"
    return hub.pull("rlm/rag-prompt")

def build_prompt_ragprompt_en():
    # This is the exact prompt from "rlm/rag-prompt" hub
    template = """
    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
    Question: {question} 
    Context: {context} 
    Answer:"""
    prompt = ChatPromptTemplate.from_template(template)
    return prompt

def build_prompt_ragprompt_vn():
    # This is the exact prompt from "rlm/rag-prompt" hub
    template = """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω chuy√™n th·ª±c hi·ªán c√°c nhi·ªám v·ª• h·ªèi-ƒë√°p. H√£y s·ª≠ d·ª•ng nh·ªØng ƒëo·∫°n ng·ªØ c·∫£nh ƒë∆∞·ª£c truy xu·∫•t sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, ch·ªâ c·∫ßn n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt. Tr·∫£ l·ªùi t·ªëi ƒëa ba c√¢u v√† gi·ªØ cho c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn.
    Question: {question} 
    Context: {context} 
    Answer:"""
    prompt = ChatPromptTemplate.from_template(template)
    return prompt

def build_prompt_ragprompt_withhistory_en():
    # This is the exact prompt from "rlm/rag-prompt" hub


    template = """
    You are an assistant for question-answering tasks. Use the following pieces of retrieved context and conversation history to answer the question. If you don't know the answer, just say that you don't know. 
    Instructions:
    - Use three sentences maximum
    - Keep the answer concise

    Conversation history:
    {chat_history}
    
    Context:
    {context} 

    Question: {question} 

    Answer:"""
    prompt = ChatPromptTemplate.from_template(template)
    return prompt

def build_prompt_v2():
    # Build a custom prompt for the RAG chain
    template = """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, chuy√™n tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.
    Context:
    {context}
    
    H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau ƒë√¢y m·ªôt c√°ch ng·∫Øn g·ªçn v√† ch√≠nh x√°c:
    {question}
    
    Tr·∫£ l·ªùi c·ªßa b·∫°n n√™n d·ª±a tr√™n th√¥ng tin trong t√†i li·ªáu v√† kh√¥ng ƒë∆∞·ª£c th√™m b·∫•t k·ª≥ th√¥ng tin n√†o kh√¥ng c√≥ trong ƒë√≥.
    """
    prompt = ChatPromptTemplate.from_template(template)
    return prompt

def retrieve_chat_history():
    # Retrieve the last x messages from chat history
    message_threshold = 10  # Number of messages to retrieve
    return st.session_state.chat_history[-message_threshold:-1]

#@st.cache_resource
def process_pdf(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, prefix = uploaded_file.name, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
    
    loader = PyPDFLoader(tmp_file_path)
    documents = loader.load()
    
    semantic_splitter = SemanticChunker(
        embeddings=st.session_state.embeddings,
        buffer_size=1,
        breakpoint_threshold_type="percentile", 
        breakpoint_threshold_amount=95,
        min_chunk_size=500,
        add_start_index=True
    )
    
    docs = semantic_splitter.split_documents(documents)
    # Fix: Use ephemeral ChromaDB client to avoid tenant error
    # client = chromadb.EphemeralClient()
    client = get_chroma_client(allow_reset=True)
    client.reset()  # Reset client to ensure no previous state
    vector_db = Chroma.from_documents(
        documents=docs, 
        embedding=st.session_state.embeddings,
        client=client
    )
    retriever = vector_db.as_retriever()
    
    #prompt = hub.pull("rlm/rag-prompt")
    prompt = build_prompt_ragprompt_withhistory_en()

    def format_docs(docs):
        logger.info(f"**Debug: Retrieved {len(docs)} chunks:**")
        for i, doc in enumerate(docs):
            # Extract metadata if available
            # Assuming each doc has metadata with 'page' and 'source'
            page_num = doc.metadata.get('page') + 1 if 'page' in doc.metadata else -1
            source = doc.metadata.get('source', 'document')
            file_name = os.path.basename(source) if isinstance(source, str) else 'unknown'

            logger.info(f"""
            ([reference-{i+1}] Page {page_num} - Source: {file_name})
            {doc.page_content}""")
        
        return "\n\n".join(doc.page_content for doc in docs)
    
    def format_history(histories):
        formatted_history = ""
        for msg in histories:
            role = "User" if msg["role"] == "user" else "Assistant"
            formatted_history += f"{role}: {msg['content']}\n\n"
        return formatted_history.strip()
    
    
    rag_chain = (
        {
            "context": itemgetter("question") | retriever | format_docs,
            #"question": RunnablePassthrough(),
            "question": itemgetter("question"),
            "chat_history": lambda x: format_history(x["chat_history"])
        }
        | prompt
        | st.session_state.llm
        | StrOutputParser()
    )
    
    os.unlink(tmp_file_path)
    return rag_chain, len(docs)

def add_message(role, content):
    """Th√™m tin nh·∫Øn v√†o l·ªãch s·ª≠ chat"""
    st.session_state.chat_history.append({
        "role": role,
        "content": content,
        "timestamp": time.time()
    })

def clear_chat():
    """X√≥a l·ªãch s·ª≠ chat"""
    st.session_state.chat_history = []

def display_chat():
    """Hi·ªÉn th·ªã l·ªãch s·ª≠ chat"""
    if st.session_state.chat_history:
        for message in st.session_state.chat_history:
            if message["role"] == "user":
                with st.chat_message("user"):
                    st.write(message["content"])
            else:
                with st.chat_message("assistant"):
                    st.write(message["content"])
    else:
        with st.chat_message("assistant"):
            st.write("Xin ch√†o! T√¥i l√† AI assistant. H√£y upload file PDF v√† b·∫Øt ƒë·∫ßu ƒë·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung t√†i li·ªáu nh√©! üòä")

# UI
def main():
    st.set_page_config(
        page_title="PDF RAG Chatbot", 
        layout="wide",
        initial_sidebar_state="expanded"
    )
    st.title("PDF RAG Assistant")

    # Trong streamlit v-1.38 kh√¥ng kh·ªó tr·ª£ param size
    st.logo("./assets/logo.png", size="large")
    
    # Sidebar
    with st.sidebar:
        st.title("‚öôÔ∏è C√†i ƒë·∫∑t")
        
        # Load models
        if not st.session_state.models_loaded:
            st.warning("‚è≥ ƒêang t·∫£i models...")
            with st.spinner("ƒêang t·∫£i AI models..."):
                st.session_state.embeddings = load_embeddings()
                st.session_state.llm = load_llm()
                st.session_state.models_loaded = True
            st.success("‚úÖ Models ƒë√£ s·∫µn s√†ng!")
            st.rerun()
        else:
            st.success("‚úÖ Models ƒë√£ s·∫µn s√†ng!")

        st.markdown("---")
        
        # Upload PDF
        st.subheader("üìÑ Upload t√†i li·ªáu")
        uploaded_file = st.file_uploader("Ch·ªçn file PDF", type="pdf")
        
        if uploaded_file:
            if st.button("üîÑ X·ª≠ l√Ω PDF", use_container_width=True):
                with st.spinner("ƒêang x·ª≠ l√Ω PDF..."):
                    st.session_state.rag_chain, num_chunks = process_pdf(uploaded_file)
                    st.session_state.pdf_processed = True
                    st.session_state.pdf_name = uploaded_file.name
                    # Reset chat history khi upload PDF m·ªõi
                    clear_chat()
                    add_message("assistant", f"‚úÖ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng file **{uploaded_file.name}**!\n\nüìä T√†i li·ªáu ƒë∆∞·ª£c chia th√†nh {num_chunks} ph·∫ßn. B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu ƒë·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung t√†i li·ªáu.")
                st.rerun()
        
        # PDF status
        if st.session_state.pdf_processed:
            st.success(f"üìÑ ƒê√£ t·∫£i: {st.session_state.pdf_name}")
        else:
            st.info("üìÑ Ch∆∞a c√≥ t√†i li·ªáu")
            
        st.markdown("---")
        
        # Chat controls
        st.subheader("üí¨ ƒêi·ªÅu khi·ªÉn Chat")
        if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat", use_container_width=True):
            clear_chat()
            st.rerun()
            
        st.markdown("---")
        
        # Instructions
        st.subheader("üìã H∆∞·ªõng d·∫´n")
        st.markdown("""
        **C√°ch s·ª≠ d·ª•ng:**
        1. **Upload PDF** - Ch·ªçn file v√† nh·∫•n "X·ª≠ l√Ω PDF"
        2. **ƒê·∫∑t c√¢u h·ªèi** - Nh·∫≠p c√¢u h·ªèi trong √¥ chat
        3. **Nh·∫≠n tr·∫£ l·ªùi** - AI s·∫Ω tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung PDF
        """)

    # Main content
    st.markdown("*Tr√≤ chuy·ªán v·ªõi Chatbot ƒë·ªÉ trao ƒë·ªïi v·ªÅ n·ªôi dung t√†i li·ªáu PDF c·ªßa b·∫°n*")
    
    # Chat container
    chat_container = st.container()
    
    with chat_container:
        # Display chat history
        display_chat()
    
    # Chat input
    if st.session_state.models_loaded:
        if st.session_state.pdf_processed:
            # User input
            user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...")
            
            if user_input:
                # Add user message
                add_message("user", user_input)
                
                # Display user message immediately
                with st.chat_message("user"):
                    st.write(user_input)
                
                # Generate response
                with st.chat_message("assistant"):
                    with st.spinner("ƒêang suy nghƒ©..."):
                        try:
                            # output = st.session_state.rag_chain.invoke(user_input)
                            output = st.session_state.rag_chain.invoke({
                                "question": user_input,
                                "chat_history": retrieve_chat_history()
                            })
                            # Clean up the response
                            if 'Answer:' in output:
                                answer = output.split('Answer:')[1].strip()
                            else:
                                answer = output.strip()
                            
                            # Display response
                            st.write(answer)
                            
                            # Add assistant message to history
                            add_message("assistant", answer)
                            
                        except Exception as e:
                            logger.error(e, exc_info=True)
                            error_msg = f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}"
                            st.error(error_msg)
                            add_message("assistant", error_msg)
        else:
            st.info("üîÑ Vui l√≤ng upload v√† x·ª≠ l√Ω file PDF tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu chat!")
            st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...", disabled=True)
    else:
        st.info("‚è≥ ƒêang t·∫£i AI models, vui l√≤ng ƒë·ª£i...")
        st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...", disabled=True)

if __name__ == "__main__":
    main()
